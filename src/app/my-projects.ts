import { Project } from './project';

export const PROJECTS: Project[] = [


{ id: 1, 
	name: 'Mind-Full',
  thumbname: 'Neurofeedback',
  class: 'mind-full',
  thumbnail:'assets/img/mindfull/mind-full-thumbnail-min.jpg',
  summary:'A series of EEG neurofeedback games to help children self-regulate' ,
  overview: 'A project to redesign and evaluate Mind-Full neurofeedback games',
  team: 'Elgin-Skye McLaren, Alissa Antle, Perry Tan, Srilekha Sridharan, Fan Lin, Emily Cramer',
  tags: 'NeuroSky Mindwave Headset, Unity Game Engine, Mobile Development',
  role: 'Project Manager, Tester, Researcher – details below',
  caption1:'A redesigned Mind-Full menu',
  caption2:'A new game to help kids relax',
  challengeHeading:'Adapting an existing tool for new users.',
  c1:'Children in the US and Canada are more likely to suffer from anxiety and attentional challenges than any other mental health issue. These children often have difficulty staying calm and focused at school. The Mind-Full application measures brain activity and provides feedback using a mobile application. It helps children learn to calm down and focus so they can do it on their own.',
  c2:'Mind-Full was developed for traumatized children in Nepal who struggled in school. Our work showed promise – students who used Mind-Full were better able to focus in class. We wondered: Could this technology be adapted to help North American children with anxiety and attentional challenges?',
  c3: 'To find out, our team needed to redesign and re-evaluate our existing prototype.',
  c4: '',
  caption3:'Brainstorming at Interaction Design and Children Conference',
  caption4:'Development in Unity Game Engine',
  problemHeading1:'Designing new interfaces',
  problemHeading2:'Evaluating the system',
  p1:'In consultation with two communities, we produced two new interfaces. The first was developed with a local First Nations community and featured wildlife imagery, such as bears, ravens, and salmon. The second, for a nearby school district, featured a silly squirrel. An in-depth analysis of the design process was recently accepted for publication in a top HCI journal – Transactions on Computer Human Interaction. ',
  p2:'We initially planned to simply reskin the Nepal prototype; however, we also wanted to improve usability. This required multiple design iterations and new functionality, such as the ability to delete user accounts and track progress. Significant testing was required to ensure Mind-Full worked across different devices, and that headset measurements were accurate. ',
  p3:'I contributed throughout the ideation and design process, and was particularly helpful during testing and development. I worked closely with another developer to find and fix issues and bugs in the games. ',
  p4:'In Fall 2016, I ran a controlled study with 25 adults to better understand the short term-effects of the redesigned Mind-Full. We presented this research at the Brain Computer Interface Conference 2017 in Graz, Austria.',
  p5:'Most recently, in 2017, we ran a 14-week waitlist-controlled study with 28 children ages 6-8 with anxiety and attentional challenges in a local school district. For this study, I was involved in study design, data collection, and analysis. Participants showed improvement, particularly related when it came to reducing anxiety (the manuscript with final results is forthcoming).',
  caption5:'Child using Mind-Full',
  caption6:'Presenting at Neuroscience 2017',
  resultHeading1: 'Co-design with children',
  resultHeading2: 'Neurofeedback for youth',

  r1:'For my master’s research, I’m working with grade 3 students as design partners to incorporate sound into the Mind-Full system. Together, we “co-design” sounds to improve user experience. This work will contribute knowledge about the impact of audio feedback on children\'s (relaxed vs. anxious) brain states, and produce an enhanced Mind-Full system for use in future studies. Data collection will be completed in spring 2017.',
  r2:' ',
  r3:'I am currently exploring other neurofeedback system designs, with the aim of creating a neurofeedback tool for youth. At BrainHack DC 2017, I worked with a team to create a brainwave visualization tool using an OpenBCI Cyton 8 channel EEG headset, Node.js, and p5.js. This work improved my understanding of UDP sockets and handling time-series data. We presented this work at the November DataViz DC meetup.',
  caption7:'BrainHack DC Neurofeedback Demo',
 url:"",
  urltext:"",
},
{ id: 2, 
	name: 'Block Talks', 
  thumbname: 'AR for Literacy',
  class: 'block-talks',
  thumbnail:'assets/img/blocktalks/block-talks-thumbnail-min.jpg',
  summary:'An augmented reality toolkit to help children learn to read',
  overview: 'An award-winning project developed during Eduhacks Hackathon 2017',
  team: 'Min Fan, Elgin-Skye McLaren, Uddipana Baishya, Shubhra Sarkar, and Amal Vincent',
  tags: 'Augmented Reality, Unity Game Engine, Tangible Computing, Games for Education',
  role: 'Ideation, Development, Project Pitch – details below', 
  caption1:'Example of tangible blocks',
  caption2:'Users Scan blocks to get feedback',
  challengeHeading:'Affordable Educational AR for schools',
  c1:'One in 10 children has trouble learning to read and spell. Block Talks was designed with these individuals in mind. Developed during the Eduhacks 24-hour hackathon 2017, our goal was to develop a low-cost augmented reality toolkit that could be used to support classroom learning. This project placed 3rd out of 66 submissions.',
  c2:'Block Talks consists of a set of physical letters, coloured blocks, and an application running on a mobile device. Children place letters on the blocks to create words and sentences. Colored blocks are used to identify sentence structure (nouns, verbs, etc). The mobile application allows children to check their spelling and syntax. Feedback is delivered through augmented reality animations and recorded audio. ',
  c3: '',
  c4: '',
  caption3:'Assembling the prototype',
  caption4:'AR animations show sentence meaning',
  problemHeading1:'Rapid Ideation',
  problemHeading2:'Building the system',
  p1:'Our development process began with a phase of rapid ideation. We defined the following criteria: the project needed to be: 1. In line with our skill sets (VR, child-computer interaction, tangible computing); 2. Practicable (a working prototype within the time constraint); 3. Relevant (real-world application). On a whiteboard, we came up with 10 project ideas, and selected the one that best fit our criteria. ',
  p2:' Next, we focused on the necessary tools, frameworks, and potential gaps in the market of augmented reality learning tools. I supported this process, and was responsible for gathering English language curriculum ideas that would lend well to this type of tool. I also created paper prototypes of the UX design and example sentences. ',
  p3:'',
  p4:'With ideas and sketches in mind, we built the app using items from a nearby dollar store. The materials weren’t always a perfect fit – for example, the blocks were too small for young children – but they were what we had available. I helped build the physical prototype and gathered media assets (eg: the cat sprite). I recorded audio voiceovers and created a bootstrap landing page to explain our project. I pitched the project during multiple rounds of evaluation and the final presentation to the judges. I learned an unbelieve amount in such a short period of time, including some of the unique challenges of AR. For example, the change in lighting from the development area to the showcase area made object recognition with the tablet difficult, and necessitated finding additional lights. Overall, the project was a success.',
  p5:'',
  caption5:'Final Presentation to the judges',
  caption6:'Our team placed #3 out of 66 projects',
  resultHeading1: 'Iterating on the design & user testing',
  resultHeading2:'',
  r1:'We are currently in the process of developing a learning curriculum to accompany this application. In 2018, we plan to begin usability testing with children. We are also working on an improved feedback mechanism (see gif example, below) to help guide children to the correct answer. An outline of this project and future work are detailed in a short publication (currently under review). ',
  r2:'',
  r3:'', 
  caption7:'Feedback Mockup for next iteration',
   url:"",
  urltext:"",
},

{ id: 3, 
  name: 'Audio-Matic',
  thumbname: 'AI Music',
  class: 'audio-matic',
  thumbnail:'assets/img/audiomatic/audio-matic-thumbnail-min.jpg',
  summary:'A novel artificial intelligence system for generative music interaction',
  overview: 'Graduate coursework project for Artificial Intelligence in Computational Art and Design',
  team: 'Contributions by Dr. Steve Dipaolo',
  tags: 'Deep Learning (Magenta/TensorFlow), MaxMSP, Arduino Microcontroller, Tangible Computing',
  role: 'Designer, Developer',
  caption1:'Audio-Matic Logo',
  caption2:'',
  challengeHeading:'Making complex AI accessible',
  c1:'Artificial intelligence (AI) is everywhere. Yet many users are apprehensive of AI systems and unaware of how they work. Ethicists recommend users interact with, and reflect on, AI applications. With this in mind, I built an interactive generative music system, Audio-Matic, as a novel means of encouraging playful, accessible experiences with AI technology.',
  c2:'Users interact with Audio-Matic through a piano keyboard. The system responds by creating new melodies based on user input. These melodies are generated using Google’s deep learning tool Magenta trained on a midi corpus. The generated response is played back for users through a series of servo motors mounted on a stringed instrument.',
  c3: '',
  c4: '',
  caption3:'Creating sounds in MaxMSP',
  caption4:'',
  problemHeading1:'Training and Building',
  problemHeading2:'',
  p1:'For this graduate-level AI course, I wanted to learn more about deep learning. I was curious about using these tools to create generative music. After researching different deep learning frameworks, I designed a project using Magenta – a Google Brain initiative that uses TensorFlow to create art and music. ',
  p2:'I gathered a corpus of MIDI files and trained the system using the Lookback Melody RNN. Next, I used MaxMSP multimedia software to create customized call and response sounds. Finally, I used OSC to send the signals live to an Arduino microcontroller. The microcontroller uses servo motors to produce real sound on real instruments. Working with physical instruments was a challenge. For example, I developed a prototype that used mallets to hit a xylophone. But the motors couldn’t recoil fast enough, and produced only a faint sound. In the end, I tried running the system on a ukulele. The strings are far enough apart that a servo can play a single note at a time.  The prototype works quite well. It does not yet play a full scale range of notes but the system could eventually be scaled up to a larger stringed instrument, such as a harp.',
  p3: '',
  p4: '',
  p5:'',
  caption5:'Servo motors controlled by AI',
  caption6:'',
  resultHeading1:'Public Demonstration',
  r1:'Future plans include the development of a larger, more robust system for public presentation. A paper documenting the Audio-Matic system can be viewed ',
  r2:"",
  r3: '',
  resultHeading2: '',
  caption7:'',
  url:"https://drive.google.com/file/d/1R1ZWA7LxxRBO8bE4EM-MS-V8vsfCNLfX/view?usp=sharing",
  urltext:"online.",

},
{ id: 4, 
  name: 'Noise Makers', 
  thumbname: 'DIY Instruments',
  class: 'tangibles',
  thumbnail:'assets/img/tangibles/tangibles-thumbnail-min.jpg',
  summary:'A series of DIY music controllers to reduce barriers to musical performance',
  overview: 'Inspired by my work with Rock Camp for Girls, I designed a workshop and a series of interactive instruments to help children and youth play with sound',
  team: 'Contributions by Rock Camp for Girls Montreal & New York',
  tags: 'Microcontrollers, Node.js, Sensors, Tangible Computing, MaxMSP, Ableton',
  role: 'Designer, Developer',
  caption1:'Montreal Rock Camp Showcase',
  caption2:'Campers preparing for Showcase',
  challengeHeading:'Overcoming Stage-Fright',
  c1:'Conventional wisdom holds that music reduces stress. Research suggests a gendered effect – girls are more likely to use music as a coping mechanism than boys. However, women are less likely to play in an extracurricular band and more likely to suffer from music performance anxiety. ',
  c2:'As a musician and volunteer at Rock Camp for Girls, I noticed many girls become self-conscious about musical performance during late childhood. This is often rooted in feelings of being judged on their musical skill and on the quality of their artistic output. To encourage kids to explore musical expression in a low-risk fashion, I started building DIY musical instruments.  I call these customizable controllers Noise Makers. I also designed a short workshop to show teens how they work.',
  c3: '',
  c4: '',
    caption3:'Instrument prototyping',
  caption4:'Audio pitch rises as object approaches',
  problemHeading1:'Mapping Sensors to Sounds',
  problemHeading2:'',
  p1:'My goal was to use affordable, open-sourced technologies to create interactive instruments that are fun and easy to use. The instruments consist of sensors (e.g.: gyroscopes, photo-sensors, ultrasonic sensors), mapped to synth instruments in MaxMSP or drum beats in Ableton Live. The sensors connect to the computer using microcontrollers such as Arduino Unos, Makey Makeys, and Lilypads for wearable sensors. For example, one instrument uses a distance sensor to control the pitch of an audio clip. As the user approaches the sensor, the pitch increases, allowing the user to play with the sound using their body. I presented this work at a tech meetup last year in a presentation called "Intro to Mad MaxMSP: Fury Code." ',
  p2:'Developing appropriate user interactions with sensors was a unique challenge and required frequent adjustments. This project was an opportunity to develop my sewing, soldering, and coding abilities. I teach these and related skills as a teaching assistant for a graduate-level course in Tangible Computing at SFU and as a hardware mentor at Vancouver’s NodeSchool. ',
  p3: '',
  p4: '',
  p5:'',  
  caption5:'Incoming data routed through Node.js',
  caption6:'Tapping real fruit alters a drum beat',
  resultHeading1:'Noise Makers Workshop',
  resultHeading2: '',
  r1:' I developed a curriculum for a 4-hour workshop, which I hope to lead at a 2018 edition of Rock Camp for Girls. Participant outcomes include knowledge of programming basics, understanding of hardware (sensors, microcontrollers, speakers), and customized instruments that participants can incorporate into their final performance.  ',
  r2:'',
  r3: '',
  caption7:'',
   url:"",
  urltext:"",
},
{ id: 5, 
  name: 'Asking for a Friend',
  thumbname: 'Health Chatbot',
  class: 'asking-for-a-friend', 
  thumbnail:'assets/img/asking/asking-for-a-friend-thumbnail-min.jpg',
  summary:'A chatbot to improve access to local mental health resources',
  overview: 'A project created for the 2017 Lumohacks Health Hackathon',
  team: 'Vivian Pan, Elgin-Skye McLaren, Joanna Zhao, Natasha Caton',
  tags: 'Node.js, Microsoft Azure Bot Framework',
  role: 'Ideation, Development',
  caption1:'Programming in Node.js',
  caption2:'',

  challengeHeading:'Barriers to Seeking Help',
  c1:'Many young people experience barriers to seeking help for mental illness. These include the absence of a confidante, shame/fear/stigma, and a lack of knowledge about available resources. Asking for a Friend is a chatbot that improves access to nearby resources – both in-person and on-the-phone. ',
  c2:'',
  c3: '',
  c4: '',

  caption3:'Rapid brainstorming notes',
  caption4:'Web Application Interface',
  problemHeading1:'Tech for Human Connections',
  problemHeading2:'',
  p1:'We wanted to create a project to improve the lives of youth with mental health challenges. We began our ideation phase by consulting a group of first-year students with firsthand experience with mental health challenges. They flagged how difficult it is to find nearby resources and how hard it is to first reach out. We conceived a chatbot that helps users connect with support organizations. ',
  p2:'Vivian Pan and I coded the bot’s back end and the web interface. Our teammates designed the conversation flow. We used Microsoft Azure’s bot service, we chose this framework because it has thorough documentation, is programed in Node.js,  and is easily be connected to social media platforms such as Facebook messenger. Further, the Azure bot service features the LUIS natural language processing tool to enable more flexible text recognition and improve user interaction. ',
  p3: 'During the hackathon, we successfully created a functioning prototype. The chatbot finds out what kind of help the user is looking for, and guides them to Vancouver-based services. ',
  p4: '',
  p5:'',

  caption5:'Overnight coding',
  resultHeading1:'Exploring Natural Language Processing',
  resultHeading2: '',
  r1:'We received excellent feedback from the Lumohacks judges and mentors on our project. I am interested in further exploring how advanced natural language processing and A/B testing can improve user experience with chatbots. ',
  r2:'',
  r3: '',
 
  caption6:'',
  caption7:'',
   url:"",
  urltext:"",

  

},
{ id: 6, 
  name: 'Virtual Earthgazing',
  thumbname: 'VR Space Travel',
  class: 'earthgazing',
  thumbnail:'assets/img/earthgazement/earthgazing-thumbnail-min.jpg',
  summary:'An exploration of whether virtual reality can help people feel more connected to the Earth',
  overview: 'A research project to investigate the potential of virtual reality to enable individuals to undergo the overview effect - an awe-inspiring experience of seeing Earth from space',
  team: 'SIAT\'s iSpace lab - Bernard Riecke, Katerina Stepanova, Denise Quesnel, Alex Kitson, Mirjana Prpa, Ivan Aguilar, Elgin-Skye McLaren',
  tags: 'Virtual Reality, HTC Vive, Unity Game Engine',
  role: 'Researcher - details below',
  challengeHeading:'Inspiring awareness of the world',
   caption1:'Can VR elicit the overview effect?',
  caption2:'',
  c1:'Astronauts often report a shift in worldview upon seeing Earth from space. The awe-inspired recognition of our planet as a fragile, and deeply connected place, is referred to as the “overview effect.” This project stemmed from the question of whether we could simulate this shift in awareness using virtual reality technology.',
  c2:'',
  c3: '',
  c4: '',
  caption3:'The experience was created for HTC Vive',
  caption4:'',
  problemHeading1:'Measuring Sensations of Awe',
  problemHeading2:'',
  p1:'I worked with members of the iSpace team to lead a research study in late 2017. I contributed to experimental design, study logistics, equipment prep, and also worked with participants. As a researcher, I welcomed participants, asked them to complete a pre-study survey, and set them up in the VR equipment. We used an HTC Vive, a custom-built chair for navigating the environment, biosensors, and a camera to record and measure the onset of goosebumps. Once the user was prepared, we ran a 15-minute, custom VR experience that simulated going into space. Following the experience, I led qualitative, cued-recall interviews to discover insights about how users felt over the course of the activity. In particular, we determined which moments participants found most memorable, whether they experienced “awe”, and documented usability issues within the game.  ',
  p2:'',
  p3: '',
  p4: '',
  p5:'',
  caption5:'Our animated experience (not pictured) was inspired by realistic experiences like Google Earth VR',
  caption6:'',
  resultHeading1:'Continued research',
  resultHeading2: '',
  r1:'Data analysis is ongoing, and many users reported a shift in awareness about the Earth after the VR experience – similar to what astronauts report. I will be assisting with future research studies, and a public exhibition in partnership with a local museum in 2018.  ',
  r2:'',
  r3: '',
  caption7:'',
   url:"",
  urltext:"",
}
];
